王煜全 人工智能芯片的现状和未来
起因：传统框架无法满足人工智能需求——CPU计算架构是串行处理，但AI是模仿大脑采用超低能耗的并行处理。对于上亿次的并行，传统计算机构无法完成。

目前：CPU加异构计算。核心部分采用传统CPU，将大量可重复的简单计算分离出来采用并行处理结构。有下面几种——

1. CPU + GPU。阿法狗怼李世石时动用了1920个CPU和280个GPU。

CPU需求大，而且必须有一定的并行能力。Intel提出了MIC（Many Integrated Core）多个集成核心架构。GPU首当其冲就是就是Nvidia，可谓是乘这股风破了千层浪。AMD也在追。

2. CPU+FPGA。大疆就是用的这个，好处是体积小，适合移动计算。合作公司是Lattice。

当下FPGA全球有两大两小：两大是Xilinx（50%）和Altera（30%），两小是Lattice和？Microsemi？

3. CPU +ＡＳＩＣ（为专门目的设计的集成电路）目的就是效率高和省电。

比如Ｍｏｖｉｄｉｕｓ提出的ＶＰＵ（Visual Processor Unit）专做视觉处理。

Mobileye专为自动驾驶的人工智能芯片。阿法狗开发出TPU（Tensor Processing Unit）专门配合Tensorflow，是算法最优化。

4. CPU+DSP（DIgital Signal Processor）

适合处理流媒体信号，适合手机。

例如 TI，Freescale，CEVA。

未来趋势：

Google的TPU和Nvidia的Tesla，都是针对人工智能芯片。

尽管现在还是属于ASIC。

未来的主流可能会是Neuromorphic Computing. 由加州理工的Carver Mead提出，现在由IBM和高通研发。整个思路就是去掉CPU部分，使处理部分完全改为并行结构。

IBM推出了TrueNorth产品，实现了几百万个计算点。芯片没有采用传统冯诺依曼结构，而把内存处理器单元和通信部件完全集成。信息处理完全本地化。内存与CPU间的瓶颈消失，同事神经元之间方便相互沟通，实现事件驱动的异步电路特征。由于不需要同步时钟，该芯片功耗极低。

虽然热工智能芯片看似蓝海，并且一旦成熟将淘汰现在所有电脑。但实际需要大量人力物力科研力量，不适合初创公司。我们可能要学伽利略，对这个工具持续关注，和自己的应用相结合，做出专用性格强的产品。
